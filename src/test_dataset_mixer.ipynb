{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import random\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "# from transformers import AutoModelForCausalLM, set_seed\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import load_dataset, load_from_disk, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = '../recipes/zephyr-7b-gemma/dpo/config_full.yaml'\n",
    "\n",
    "# with open(file, 'r') as f:\n",
    "#     args = yaml.safe_load(f)\n",
    "\n",
    "# parser = H4ArgumentParser((ModelArguments, DataArguments, DPOConfig))\n",
    "# model_args, data_args, training_args = parser.parse_dict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_mixer': ['argilla/dpo-mix-7k',\n",
       "  {'argilla/dpo-mix-7k': {'split': {'train': 'train', 'test': 'test'},\n",
       "    'chat_template': 'chatml'}},\n",
       "  {'argilla/dpo-mix-7k': {'chat_template': 'chatml'}},\n",
       "  {'/home/kenyo/workspace/LLM-Trainer/dataset/LukasSonn/DoxygenStrings-Long': {'split': {'train': 'train',\n",
       "     'test': 'test'}}},\n",
       "  {'/home/kenyo/workspace/LLM-Trainer/dataset/LukasSonn/DoxygenStrings-Long': {'split': {'train': 'train.json',\n",
       "     'test': 'test.json'}}}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '/home/kenyo/workspace/LLM-Trainer/recipes/dataset_mixer.yaml'\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    args = yaml.safe_load(f)\n",
    "\n",
    "dataset_mixer = args\n",
    "dataset_mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetSplit:\n",
    "    train:str\n",
    "    test:str\n",
    "\n",
    "@dataclass\n",
    "class DatasetGroupbySplit:\n",
    "    train :list = field(default_factory=lambda: [])\n",
    "    test  :list = field(default_factory=lambda: [])\n",
    "\n",
    "class DatasetMixer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dataset_groupby_split = DatasetGroupbySplit()\n",
    "\n",
    "\n",
    "    def _load(self, dataset_name_or_path:str, split:dict=None):\n",
    "        if split is None:\n",
    "            dataset_split = DatasetSplit(train='train', test='test')\n",
    "        elif isinstance(split, list):\n",
    "            dataset_split = DatasetSplit(**{_split:_split for _split in split})\n",
    "        else:\n",
    "            dataset_split = DatasetSplit(**split)\n",
    "\n",
    "        for key, _ in dataset_split.__dict__.items():\n",
    "            split = getattr(dataset_split, key)\n",
    "            dataset_group = getattr(self.dataset_groupby_split, key)\n",
    "\n",
    "            if os.path.exists(dataset_name_or_path):\n",
    "\n",
    "                if _ext:= os.path.splitext(split)[-1].lstrip('.'):\n",
    "                    _file = os.path.join(dataset_name_or_path, split)\n",
    "                    dataset_subgroup = load_dataset(_ext, data_files={key:_file})[key]\n",
    "                    print(f'Successfully loaded dataset {dataset_name_or_path} from local through \\'{_ext}\\' format.')\n",
    "                else:\n",
    "                    dataset_subgroup = load_from_disk(os.path.join(dataset_name_or_path, split))\n",
    "                    print(f'Successfully loaded dataset {dataset_name_or_path} from local through \\'datasets\\' format.')\n",
    "            else:\n",
    "                dataset_subgroup = load_dataset(dataset_name_or_path, split=split)\n",
    "                print(f'Successfully loaded dataset {dataset_name_or_path} from HuggingFace Hub or ~/.cache.')\n",
    "\n",
    "            dataset_group.append(dataset_subgroup)\n",
    "\n",
    "        return self.dataset_groupby_split\n",
    "\n",
    "\n",
    "    def load_and_mix(self, dataset_mixer:list):\n",
    "\n",
    "        if isinstance(dataset_mixer, list):\n",
    "\n",
    "            for config in dataset_mixer:\n",
    "\n",
    "                if isinstance(config, str):\n",
    "                    dataset_name_or_path = config\n",
    "\n",
    "                    print(f'Load dataset {dataset_name_or_path} with default configurations.')\n",
    "                    self._load(dataset_name_or_path)\n",
    "\n",
    "                elif isinstance(config, dict):\n",
    "                    dataset_name_or_path = list(config.keys())[0]\n",
    "                    config = config[dataset_name_or_path]\n",
    "                    split = config.get('split', None)\n",
    "\n",
    "                    print(f'Load dataset {dataset_name_or_path} with following configurations.')\n",
    "                    for key, value in config.items():\n",
    "                        print(f'    - {key}: {value}')\n",
    "                    self._load(dataset_name_or_path, split=split)\n",
    "\n",
    "        return self.dataset_groupby_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['argilla/dpo-mix-7k',\n",
       " {'argilla/dpo-mix-7k': {'split': {'train': 'train', 'test': 'test'},\n",
       "   'chat_template': 'chatml'}},\n",
       " {'argilla/dpo-mix-7k': {'chat_template': 'chatml'}},\n",
       " {'/home/kenyo/workspace/LLM-Trainer/dataset/LukasSonn/DoxygenStrings-Long': {'split': {'train': 'train',\n",
       "    'test': 'test'}}},\n",
       " {'/home/kenyo/workspace/LLM-Trainer/dataset/LukasSonn/DoxygenStrings-Long': {'split': {'train': 'train.json',\n",
       "    'test': 'test.json'}}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '/home/kenyo/workspace/LLM-Trainer/recipes/dataset_mixer.yaml'\n",
    "with open(file, 'r') as f:\n",
    "    args = yaml.safe_load(f)\n",
    "\n",
    "dataset_config = args['dataset_mixer']\n",
    "dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset argilla/dpo-mix-7k with default configurations.\n",
      "Successfully loaded dataset argilla/dpo-mix-7k from HuggingFace Hub or ~/.cache.\n",
      "Successfully loaded dataset argilla/dpo-mix-7k from HuggingFace Hub or ~/.cache.\n",
      "Load dataset argilla/dpo-mix-7k with following configurations.\n",
      "    - split: {'train': 'train', 'test': 'test'}\n",
      "    - chat_template: chatml\n",
      "Successfully loaded dataset argilla/dpo-mix-7k from HuggingFace Hub or ~/.cache.\n",
      "Successfully loaded dataset argilla/dpo-mix-7k from HuggingFace Hub or ~/.cache.\n",
      "Load dataset argilla/dpo-mix-7k with following configurations.\n",
      "    - chat_template: chatml\n",
      "Successfully loaded dataset argilla/dpo-mix-7k from HuggingFace Hub or ~/.cache.\n",
      "Successfully loaded dataset argilla/dpo-mix-7k from HuggingFace Hub or ~/.cache.\n",
      "Load dataset /home/kenyo/workspace/LLM-Trainer/dataset/LukasSonn/DoxygenStrings-Long with following configurations.\n",
      "    - split: {'train': 'train', 'test': 'test'}\n",
      "Successfully loaded dataset /home/kenyo/workspace/LLM-Trainer/dataset/LukasSonn/DoxygenStrings-Long from local through 'datasets' format.\n",
      "Successfully loaded dataset /home/kenyo/workspace/LLM-Trainer/dataset/LukasSonn/DoxygenStrings-Long from local through 'datasets' format.\n",
      "Load dataset /home/kenyo/workspace/LLM-Trainer/dataset/LukasSonn/DoxygenStrings-Long with following configurations.\n",
      "    - split: {'train': 'train.json', 'test': 'test.json'}\n",
      "Successfully loaded dataset /home/kenyo/workspace/LLM-Trainer/dataset/LukasSonn/DoxygenStrings-Long from local through json format.\n",
      "Successfully loaded dataset /home/kenyo/workspace/LLM-Trainer/dataset/LukasSonn/DoxygenStrings-Long from local through json format.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetGroupbySplit(train=[Dataset({\n",
       "    features: ['dataset', 'chosen', 'rejected', 'chosen_rating', 'rejected_rating'],\n",
       "    num_rows: 6750\n",
       "}), Dataset({\n",
       "    features: ['dataset', 'chosen', 'rejected', 'chosen_rating', 'rejected_rating'],\n",
       "    num_rows: 6750\n",
       "}), Dataset({\n",
       "    features: ['dataset', 'chosen', 'rejected', 'chosen_rating', 'rejected_rating'],\n",
       "    num_rows: 6750\n",
       "}), Dataset({\n",
       "    features: ['question', 'context', 'answer'],\n",
       "    num_rows: 10235\n",
       "}), Dataset({\n",
       "    features: ['context', 'answer', 'question'],\n",
       "    num_rows: 10235\n",
       "})], test=[Dataset({\n",
       "    features: ['dataset', 'chosen', 'rejected', 'chosen_rating', 'rejected_rating'],\n",
       "    num_rows: 750\n",
       "}), Dataset({\n",
       "    features: ['dataset', 'chosen', 'rejected', 'chosen_rating', 'rejected_rating'],\n",
       "    num_rows: 750\n",
       "}), Dataset({\n",
       "    features: ['dataset', 'chosen', 'rejected', 'chosen_rating', 'rejected_rating'],\n",
       "    num_rows: 750\n",
       "}), Dataset({\n",
       "    features: ['question', 'context', 'answer'],\n",
       "    num_rows: 2047\n",
       "}), Dataset({\n",
       "    features: ['context', 'answer', 'question'],\n",
       "    num_rows: 2047\n",
       "})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mixer = DatasetMixer()\n",
    "dataset_groupby_split = dataset_mixer.load_and_mix(dataset_config)\n",
    "dataset_groupby_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chromadb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
